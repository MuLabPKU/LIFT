{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773ee206",
   "metadata": {},
   "source": [
    "# Quick-Start Example\n",
    "\n",
    "Follow the instructions below to apply LIFT on your models!\n",
    "\n",
    "**⚠ Notice:** Do not run the notebook directly, since we adopt `accelerate` for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128975e",
   "metadata": {},
   "source": [
    "## 1 Load the model to train\n",
    "\n",
    "LIFT fine-tunes a pretrained LLM using LoRA. Empirically, it's sufficient to train a 7~8B model like `Llama-3-8B-Instruct` using LoRA with r=8 on a GPU of 80GB memory, so we just adopt data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6573a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take Llama 3 as an example here.\n",
    "# Replace it with your own model path or name.\n",
    "MODEL_NAME_OR_PATH = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "from lift.model import load_training_model\n",
    "\n",
    "# A util function to load a model with LoRA adapter for training.\n",
    "# We prepare the model for subsequent fine-tuning. You can check the implementation for details.\n",
    "model = load_training_model(MODEL_NAME_OR_PATH, adapter=\"lora\", adapter_config=\"configs/adapter/lora_128.yaml\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc06714",
   "metadata": {},
   "source": [
    "## 2 Prepare the dataset\n",
    "\n",
    "We use an LLM server for data generation. Launch the vLLM server first. You can also adopt DeepSeek as the generator backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f723d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# This is the script we used in our experiments.\n",
    "# You can adjust the parameters as needed.\n",
    "vllm serve Qwen/Qwen2.5-72B-Instruct \\\n",
    "    --served-model-name qwen \\\n",
    "    --gpu_memory_utilization 0.96 \\\n",
    "    --tensor-parallel-size 8 \\\n",
    "    --max-model-len 4000 \\\n",
    "    --distributed-executor-backend mp \\\n",
    "    --port 8001\n",
    "\n",
    "# These environment variables are used by our data generation scripts.\n",
    "export VLLM_MODEL_NAME=\"qwen\"  # Use the --served-model-name specified above\n",
    "export VLLM_BASE_URL=\"http://localhost:8001/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50dff74",
   "metadata": {},
   "source": [
    "We also support using DeepSeek as the backend. You just need to set the environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70176e5e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export DEEPSEEK_API_KEY=\"<your_api_key_here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffcd92",
   "metadata": {},
   "source": [
    "Prepare your context using the Spacy sentence-splitting model. **⚠ Notice**: You may need to import spacy after import torch to avoid CUDA initialization issues. Please refer to our implementation `pred/loogle/pred_lift_icl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "sentence_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "context = \"<your_context_here>\"  # It should be meaningful text in natural language.\n",
    "sentences = [sent.text for sent in sentence_model(context).sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead4ee7",
   "metadata": {},
   "source": [
    "We implement `EverySentenceDataset` in our package, which is an `IterableDataset`. Once instantialized, it begins to synthesize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cf168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from lift.synqa import AsyncOpenAIServer, EverySentenceDataset\n",
    "\n",
    "accelerator = Accelerator()  \n",
    "\n",
    "server = AsyncOpenAIServer.from_config(\"configs/generator/vllm-qwen2.5-backend.yaml\")\n",
    "# Or use the following line if you adopt DeepSeek as the backend.\n",
    "# server = AsyncOpenAIServer.from_config(\"configs/generator/deepseek-backend.yaml\")\n",
    "\n",
    "dataset = EverySentenceDataset.from_config(\n",
    "    \"configs/generator/vllm-qwen2.5-everysentence.yaml\",\n",
    "    tokenizer,\n",
    "    is_main_process=accelerator.is_main_process,\n",
    "    server=server,\n",
    "    sentences=sentences,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fcedd",
   "metadata": {},
   "source": [
    "## 3 Fine-tune\n",
    "\n",
    "We adopt `transformers.Trainer` to fine-tune the model. Since we use an `IterableDataset`, we implement a callback util `EverySentenceStopCallback` to control the training process -- training for exact `num_train_epochs` epochs.\n",
    "\n",
    "Besides, since the dataset generates training data on-the-fly, we only allow the main process to generate training data and distribute to other processes, by setting `accelerator_config={\"dispatch_batches\": True}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5562927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from lift.synqa import EverySentenceStopCallback\n",
    "from lift.utils import custom_2_hf_training_args, WrapperForIterableDataset, LIFTDataCollator\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# We set many default arguments here. You can modify them as needed.\n",
    "with open(\"configs/training/lift_lora.yaml\", \"r\") as f:\n",
    "    training_args = yaml.safe_load(f)\n",
    "training_args[\"per_device_train_batch_size\"] *= torch.cuda.device_count()\n",
    "training_args = custom_2_hf_training_args(\n",
    "    training_args,\n",
    "    accelerator_config={\"split_batches\": True, \"dispatch_batches\": True, \"even_batches\": True},\n",
    "    max_steps=int(10000),  # sufficiently large -- the stop of training is controlled by `control_callback`\n",
    "    use_liger_kernel=True,\n",
    ")\n",
    "real_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "\n",
    "data_collator = LIFTDataCollator(tokenizer)\n",
    "control_callback = EverySentenceStopCallback(dataset, training_args.num_train_epochs, real_batch_size)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=WrapperForIterableDataset(dataset, real_batch_size),  # The wrapper is used for padding the dataset to make it divisible by batch size.\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[control_callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f816a5",
   "metadata": {},
   "source": [
    "## 4 Inference\n",
    "\n",
    "After fine-tuning, the model is capable of answering context-related questions, without providing the context in its context window. Enjoy it! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8df564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lift.utils import get_pad_token_id\n",
    "\n",
    "model.eval()\n",
    "# Optional\n",
    "# model.merge_and_unload()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"<your_question_here>\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).input_ids.to(model.device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=get_pad_token_id(tokenizer),  # A util function to deal with the tokenizers without a pad token.\n",
    ")\n",
    "response = tokenizer.decode(output_ids[0, input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
